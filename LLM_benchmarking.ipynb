{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3.10 install  langchain_community langchain_experimental langchain_ollama unstructured pdf2image pdfminer open-clip-torch pylzma langchain faiss-cpu PyPDF2\n",
    "# !pip3.10 install transformers -U\n",
    "\n",
    "# !ollama pull llava-phi3\n",
    "# !ollama pull llama3.2-vision \n",
    "# !ollama pull moondream\n",
    "# !ollama pull llama3.2:1b\n",
    "# !ollama pull llama3.2\n",
    "# !ollama pull granite3.1-moe:3b\n",
    "# !ollama pull smollm2:1.7b\n",
    "# !ollama pull smollm2:1.7b-instruct-fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "# from ollama import ChatResponse\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def ask_llm(model, question, context = None, image_path_list = None):\n",
    "    res = chat(\n",
    "\tmodel=model,\n",
    "\tmessages=[{'role': 'user',\n",
    "            'content': context + question}]\n",
    "    )\n",
    "    \n",
    "    output_ = res['message']['content']\n",
    "    # return textwrap.fill(output_, width=150)\n",
    "    return output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Human-like Prompt</th>\n",
       "      <th>filename</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Array Formulas</td>\n",
       "      <td>Sum product but not regular.</td>\n",
       "      <td>Niche_Unclear_Excel_Prompts</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Automation and Macros</td>\n",
       "      <td>Can I assign a macro to a button for quick exe...</td>\n",
       "      <td>Excel_Help_Prompts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Topic                                  Human-like Prompt  \\\n",
       "66         Array Formulas                       Sum product but not regular.   \n",
       "43  Automation and Macros  Can I assign a macro to a button for quick exe...   \n",
       "\n",
       "                       filename  index  \n",
       "66  Niche_Unclear_Excel_Prompts      0  \n",
       "43           Excel_Help_Prompts      1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the prompts\n",
    "\n",
    "prompt_list = pd.read_csv(\"/Users/lekshmanannatarajan/Library/CloudStorage/GoogleDrive-zuari1993@gmail.com/My Drive/Colab Notebooks/Phi_LLM/data_local/documents/100_prompts_2.csv\", index_col = 0)\n",
    "prompt_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Prompts: 100%|██████████| 2/2 [01:19<00:00, 39.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /Users/lekshmanannatarajan/Library/CloudStorage/GoogleDrive-zuari1993@gmail.com/My Drive/Colab Notebooks/Phi_LLM/data_local/outputs/model_outputs_multiple_prompts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "context = '''\n",
    "You are an Microsoft Excel instructor who provides crisp, clear and concise instructions to user prompts.\n",
    "All your responses are 500 words or less.\n",
    "User should be able to action on the instructions and solve their problems.\n",
    "Follow the below format for your response.\n",
    "\n",
    "[START]\n",
    "Reason: a single sentence explanation of all below steps with reasoning.\n",
    "Step: mouse or keyboard actions user has to perform.\n",
    "Step: mouse or keyboard actions user has to perform.\n",
    "...\n",
    "[END]\n",
    "\n",
    "'''\n",
    "\n",
    "model_list = [\"llava-phi3\", \n",
    "            #   \"llama3.2-vision\",\n",
    "              \"moondream\",\n",
    "              \"llama3.2:1b\", \n",
    "              \"llama3.2\",\n",
    "              \"granite3.1-moe:3b\",\n",
    "            #   \"smollm2:1.7b\",\n",
    "              \"smollm2:1.7b-instruct-fp16\"\n",
    "              ]\n",
    "\n",
    "# Create a dictionary to store results\n",
    "results = {\n",
    "    \"index_\":prompt_list.index.to_list(),\n",
    "    \"topics\": prompt_list['Topic'].tolist(),\n",
    "    \"prompt\": prompt_list['Human-like Prompt'].tolist(), }\n",
    "\n",
    "# Initialize dictionary to store model outputs and times\n",
    "for model_ in model_list:\n",
    "    results[model_] = []          # For model output\n",
    "    results[f\"{model_}_time\"] = []  # For model execution time\n",
    "\n",
    "# Iterate over prompts and models\n",
    "for prompt in tqdm(prompt_list['Human-like Prompt'].tolist(), desc=\"Processing Prompts\"):\n",
    "    for model_ in tqdm(model_list, leave=False, desc=\"Processing Models\"):\n",
    "        try:\n",
    "            start_time = time.time()  # Start timing\n",
    "            output = ask_llm(model=model_, question=prompt, context=context)\n",
    "            end_time = time.time()  # End timing\n",
    "\n",
    "            elapsed_time = end_time - start_time  # Calculate time taken\n",
    "\n",
    "            # Append results for the current model and prompt\n",
    "            results[model_].append(output)\n",
    "            results[f\"{model_}_time\"].append(f\"{elapsed_time:.2f} seconds\")\n",
    "        except Exception as e:\n",
    "            # Handle any errors during model execution\n",
    "            print(f\"Error with model '{model_}' and prompt '{prompt}': {e}\")\n",
    "            results[model_].append(\"Error\")\n",
    "            results[f\"{model_}_time\"].append(\"Error\")\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_path = \"/Users/lekshmanannatarajan/Library/CloudStorage/GoogleDrive-zuari1993@gmail.com/My Drive/Colab Notebooks/Phi_LLM/data_local/outputs/model_outputs_multiple_prompts.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Results saved to {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
